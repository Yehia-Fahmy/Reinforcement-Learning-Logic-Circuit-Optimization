"""
Problem 0: Exhaustive Search and Value Functions

Your task is to implement an exhaustive search algorithm that computes the true value
function V(s) = max_a Q(s, a) for small circuit instances.

BACKGROUND:
In reinforcement learning, the value function V(s) represents the maximum expected
return achievable from state s. For our LUT optimization problem:
- States (s) represent different netlist configurations
- Actions (a) represent LUT merge operations
- The value function tells us the best possible future reward from each state

CHALLENGES TO SOLVE:
- Implement

TODO ITEMS:
- Test on small benchmarks and analyze the resulting value function

This implementation will serve as ground truth for evaluating your heuristic and RL agents.
"""

from collections import defaultdict, deque
import opt_gym.epfl as epfl
import opt_gym.envs as envs
from opt_gym.blif_parser import parse_blif_to_netlist
from opt_gym.core import LUT_ID, LUTNetlist

# Small test circuit for development and testing
blif_data = """
# Generated by Yosys 0.48+70 (git sha1 502c39b87, g++ 13.3.0-6ubuntu2~24.04 -fPIC -O3)

.model top
.inputs a[0] a[1] a[2] b[0] b[1] b[2]
.outputs sum[0] sum[1] sum[2] cout
.names $false
.names $true
1
.names $undef
.names a[1] b[1] $abc$344$aiger343$11
11 1
.names a[1] b[1] $abc$344$aiger343$14
01 1
10 1
.names $abc$344$aiger343$7 $abc$344$aiger343$14 $abc$344$aiger343$15
11 1
.names a[2] b[2] $abc$344$aiger343$19
11 1
.names a[2] b[2] $abc$344$aiger343$22
01 1
10 1
.names $abc$344$aiger343$15 $abc$344$aiger343$11 $abc$344$aiger343$23
00 1
.names $abc$344$aiger343$23 $abc$344$aiger343$22 $abc$344$aiger343$24
01 1
.names a[0] b[0] $abc$344$aiger343$7
11 1
.names a[0] b[0] sum[0]
01 1
10 1
.names $abc$344$aiger343$7 $abc$344$aiger343$14 sum[1]
01 1
10 1
.names $abc$344$aiger343$23 $abc$344$aiger343$22 sum[2]
00 1
11 1
.names $abc$344$aiger343$24 $abc$344$aiger343$19 cout
01 1
10 1
11 1
.end
"""


class ExhaustiveSearch:
    """
    Exhaustive search to find the true value function V(s) = max_a Q(s, a)
    """

    def __init__(self, env: envs.LUTEnv):
        self.initial_env = env
        self.value_function: dict[LUTNetlist, float] = {}
        self.policy: dict[LUTNetlist, LUT_ID | None] = {}
        self.visited_states: set[LUTNetlist] = set()
        self.predecessors: dict[LUTNetlist, list[LUTNetlist]] = defaultdict(list)
        self.successors: dict[LUTNetlist, list[tuple[LUT_ID, LUTNetlist]]] = (
            defaultdict(list)
        )
        self.terminal_states: set[LUTNetlist] = set()
        self.pseudo_terminal_states: set[LUTNetlist] = set()  # States at max depth

    def get_reward(self, info: envs.NetlistInfo) -> float:
        """Calculate reward based on LUTs * depth (lower is better)"""
        return -(info.num_luts * info.max_depth)

    def explore_graph(self, max_depth: int = 10):
        """Explore the state graph up to max_depth using BFS"""
        queue = deque([(self.initial_env, 0)])
        self.visited_states.add(self.initial_env.netlist)

        while queue:
            current_env, depth = queue.popleft()
            current_netlist = current_env.netlist

            if depth >= max_depth:
                # Pseudo-terminal state (reached max depth)
                self.pseudo_terminal_states.add(current_netlist)
                continue

            # Explore all possible actions
            moves = list(current_env.get_moves())
            valid_moves = False

            for move in moves:
                delta = current_env.observe_move(move)
                if delta.largest_new_k() > 6:
                    continue

                valid_moves = True
                env_copy = current_env.copy()
                env_copy.commit_move(delta)
                next_netlist = env_copy.netlist

                self.successors[current_netlist].append((move, next_netlist))
                self.predecessors[next_netlist].append(current_netlist)

                if next_netlist not in self.visited_states:
                    self.visited_states.add(next_netlist)
                    queue.append((env_copy, depth + 1))

            # Mark as terminal if no valid moves
            if not valid_moves:
                self.terminal_states.add(current_netlist)

    def compute_values(self, num_iterations: int = 50):
        """Compute value function and policy using value iteration"""
        # Initialize values randomly and policy empty
        for state in self.visited_states:
            self.value_function[state] = 0.0
            self.policy[state] = None

        # Set terminal state values (true terminals)
        for state in self.terminal_states:
            info = envs.LUTEnv(state).get_info()
            self.value_function[state] = self.get_reward(info)

        # Set pseudo-terminal state values (max depth reached)
        for state in self.pseudo_terminal_states:
            info = envs.LUTEnv(state).get_info()
            self.value_function[state] = self.get_reward(info)

        # Value iteration
        for _ in range(num_iterations):
            for state in self.visited_states:
                if (
                    state in self.terminal_states
                    or state in self.pseudo_terminal_states
                ):
                    continue

                this_info = envs.LUTEnv(state).get_info()
                this_reward = self.get_reward(info)
                best_value = float('-inf')
                best_action = None
                for move, next_state in self.successors[state]:
                    next_value = self.value_function.get(next_state, 0.0)
                    if next_state in self.terminal_states or next_state in self.pseudo_terminal_states:
                        total_value = this_reward
                    else:
                        total_value = next_value + this_reward
                    if total_value > best_value:
                        best_value = total_value
                        best_action = move
                self.value_function[state] = best_value
                self.policy[state] = best_action

    def generate_dot_graph(self, filename: str = "search_tree.dot") -> str:
        """Generate DOT graph representation of the search space"""
        dot_content = ["digraph SearchTree {"]
        dot_content.append("    rankdir=TB;")
        dot_content.append("    ranksep=5;")
        dot_content.append("    node [shape=circle];")
        dot_content.append("")

        # Helper function to get a short hash for labeling
        def get_node_label(netlist: LUTNetlist) -> str:
            return str(hash(netlist))[-6:]  # Last 6 digits of hash

        # Add nodes with appropriate styling
        for state in self.visited_states:
            label = get_node_label(state)
            info = envs.LUTEnv(state).get_info()
            tooltip = f"LUTs: {info.num_luts}, Depth: {info.max_depth}"

            # Style based on state type
            if state == self.initial_env.netlist:
                # Initial state - bright green
                style = f'[label="{label}", fillcolor=lightgreen, style=filled, tooltip="{tooltip}"]'
            elif state in self.terminal_states:
                # Terminal states - red
                style = f'[label="{label}", fillcolor=lightcoral, style=filled, tooltip="{tooltip}"]'
            elif state in self.pseudo_terminal_states:
                # Pseudo-terminal states (max depth) - orange
                style = f'[label="{label}", fillcolor=orange, style=filled, tooltip="{tooltip}"]'
            else:
                # Regular states - light blue
                style = f'[label="{label}", fillcolor=lightblue, style=filled, tooltip="{tooltip}"]'

            dot_content.append(f'    "{label}" {style};')

        dot_content.append("")

        # Add edges with move labels
        for source, move_targets in self.successors.items():
            source_label = get_node_label(source)
            optimal_move = self.policy.get(source)
            for move, target in move_targets:
                target_label = get_node_label(target)
                edge_style = ""
                if optimal_move == move:
                    edge_style = ", color=red, penwidth=2"  # Highlight optimal moves
                dot_content.append(
                    f'    "{source_label}" -> "{target_label}" [label="{move}"{edge_style}];'
                )

        dot_content.append("}")
        dot_content.append("")

        # Write to file
        dot_string = "\n".join(dot_content)
        with open(filename, "w") as f:
            f.write(dot_string)

        return dot_string

    def get_optimal_action(self, netlist: LUTNetlist) -> LUT_ID | None:
        """Get the optimal action for a given state"""
        return self.policy.get(netlist)

    def get_state_value(self, netlist: LUTNetlist) -> float:
        """Get the value of a given state"""
        return self.value_function.get(netlist, float("-inf"))


def test_small_benchmark():
    """Test exhaustive search on small benchmark"""
    print("Testing exhaustive search on small benchmark...")

    blif_netlist = parse_blif_to_netlist(blif_data)
    env = envs.LUTEnv(blif_netlist)
    searcher = ExhaustiveSearch(env)

    print(f"Initial state info:")
    info = env.get_info()
    print(f"  LUTs: {info.num_luts}, Depth: {info.max_depth}")
    print(f"  LUTs * Depth: {info.num_luts * info.max_depth}")

    print("Running exhaustive search (depth=15)...")
    searcher.explore_graph(max_depth=15)
    searcher.compute_values(num_iterations=50)

    print(f"Explored {len(searcher.value_function)} unique states")
    print("Generating search tree visualization...")
    searcher.generate_dot_graph("search_tree.dot")
    print("DOT graph saved to: search_tree.dot")

    initial_netlist = env.netlist
    optimal_action = searcher.get_optimal_action(initial_netlist)
    optimal_value = searcher.get_state_value(initial_netlist)

    print(f"Optimal action for initial state: {optimal_action}")
    print(f"Optimal value for initial state: {optimal_value}")

    # Test optimal policy
    current_env = env.copy()
    moves_taken: list[LUT_ID] = []

    print("Executing optimal policy for 10 steps...")
    print("Initial state:")
    info = current_env.get_info()
    print(f"LUTs: {info.num_luts}, Depth: {info.max_depth}")

    for step in range(10):
        optimal_move = searcher.get_optimal_action(current_env.netlist)
        if optimal_move is None:
            break

        delta = current_env.observe_move(optimal_move)
        if delta.largest_new_k() > 6:
            break

        current_env.commit_move(delta)
        moves_taken.append(optimal_move)

        info = current_env.get_info()
        print(
            f"After move {optimal_move}: LUTs={info.num_luts}, Depth={info.max_depth}"
        )

    final_info = current_env.get_info()
    print("Final state after executing optimal policy:")
    print(f"LUTs: {final_info.num_luts}, Depth: {final_info.max_depth}")
    print(f"LUTs * Depth: {final_info.num_luts * final_info.max_depth}")
    print("Summary of moves:")
    if not moves_taken:
        print("No moves taken.")
    else:
        print(f"Moves taken: {moves_taken}")

    print(f"Total states: {len(searcher.visited_states)}")
    print(f"Terminal states: {len(searcher.terminal_states)}")
    print(f"Pseudo-terminal states: {len(searcher.pseudo_terminal_states)}")


def main():
    """Main function to run problem 0"""
    test_small_benchmark()


if __name__ == "__main__":
    main()
